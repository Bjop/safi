{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:31:52.607991400Z",
     "start_time": "2024-02-11T18:31:48.802745Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# mapping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e98cef4a3519c55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "objects = [\"assembler\", \"constructor\"]\n",
    "label_map = [[1, 0], [0, 1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:31:52.612005100Z",
     "start_time": "2024-02-11T18:31:52.608995Z"
    }
   },
   "id": "c36f29e6f777370f",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d537bdc0b5ea98fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    height, width = img.shape[:2]\n",
    "\n",
    "    new_width = 1920\n",
    "    new_height = 1080\n",
    "\n",
    "    if width < new_width or height < new_height:\n",
    "        raise Exception(f\"The original image size is smaller than the specified crop size: {width}x{height}\")\n",
    "\n",
    "    start_x = (width - new_width) // 2\n",
    "    start_y = (height - new_height) // 2\n",
    "    end_x = start_x + new_width\n",
    "    end_y = start_y + new_height\n",
    "\n",
    "    cropped_img = img[start_y:end_y, start_x:end_x]\n",
    "\n",
    "    return cropped_img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:31:52.615803400Z",
     "start_time": "2024-02-11T18:31:52.613295800Z"
    }
   },
   "id": "8458e0897d50770a",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed0c5a2ecb54cb2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_images(task:str, type_object: str, machine: str):\n",
    "    images_dic = []\n",
    "    machine = os.path.join(os.getcwd(),\"data\", task, type_object, machine)\n",
    "    for subdir, dirs, files in os.walk(machine):\n",
    "        for file in files:\n",
    "            img = cv2.imread(os.path.join(machine, file))\n",
    "            if img is not None:\n",
    "                if img.shape[0] != 1080:\n",
    "                    img = crop_image(img)\n",
    "                img = np.array(img).reshape(len(img), 1920, 3)\n",
    "                images_dic.append(img)\n",
    "\n",
    "    return np.array(images_dic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:31:57.242352300Z",
     "start_time": "2024-02-11T18:31:57.238850300Z"
    }
   },
   "id": "82b77f342f72ac2a",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# preparing data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74e0f473bb87b299"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_data(task:str):\n",
    "    objects = [\"assembler\", \"constructor\"]\n",
    "    label_map = [[1, 0], [0, 1]]\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(len(objects)):\n",
    "        images = get_images(task, \"machine\", objects[i])\n",
    "\n",
    "        for image in images:\n",
    "            img = np.array(image).reshape(len(image), 1920, 3)\n",
    "            data.append(np.array(img))\n",
    "            labels.append(label_map[i])\n",
    "\n",
    "    return np.array(data), labels\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:31:58.710911100Z",
     "start_time": "2024-02-11T18:31:58.707899300Z"
    }
   },
   "id": "822a6b5ff1648e64",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# creating model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f1c887f9468206f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(1080, 1920, 3)\n",
    ")\n",
    "\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomContrast(.2),\n",
    "    layers.RandomBrightness(.1),\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "])\n",
    "\n",
    "def create_simpleconvnet():\n",
    "    inputs = keras.Input(shape=(1080, 1920, 3))\n",
    "    # x = layers.Rescaling(1./255)(inputs)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = keras.applications.vgg16.preprocess_input(x)\n",
    "    x = conv_base(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\",optimizer=keras.optimizers.Adam(learning_rate=1e-6), metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:32:01.836491700Z",
     "start_time": "2024-02-11T18:32:01.453154300Z"
    }
   },
   "id": "3953ff31732c6fe2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1080, 1920, 3)]   0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1080, 1920, 3)     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem_1  (None, 1080, 1920, 3)     0         \n",
      "  (SlicingOpLambda)                                              \n",
      "                                                                 \n",
      " tf.nn.bias_add_1 (TFOpLamb  (None, 1080, 1920, 3)     0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 33, 60, 512)       14714688  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1013760)           0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               519045632 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 533826242 (1.99 GB)\n",
      "Trainable params: 523831170 (1.95 GB)\n",
      "Non-trainable params: 9995072 (38.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_simpleconvnet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T16:18:56.793613400Z",
     "start_time": "2024-02-11T16:18:55.841588400Z"
    }
   },
   "id": "5ebf20cdb1078339",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55f27f1a2f0f2050"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n",
      "[0, 1]\n",
      "Epoch 1/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 8.6318 - accuracy: 0.5938 INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 372s 16s/step - loss: 8.6318 - accuracy: 0.5938 - val_loss: 3.4977 - val_accuracy: 0.6964\n",
      "Epoch 2/100\n",
      "23/23 [==============================] - 353s 15s/step - loss: 5.2218 - accuracy: 0.7366 - val_loss: 3.7336 - val_accuracy: 0.6786\n",
      "Epoch 3/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 5.6788 - accuracy: 0.6964 INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 354s 16s/step - loss: 5.6788 - accuracy: 0.6964 - val_loss: 1.8819 - val_accuracy: 0.8571\n",
      "Epoch 4/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 4.3360 - accuracy: 0.7277 INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 379s 17s/step - loss: 4.3360 - accuracy: 0.7277 - val_loss: 1.3376 - val_accuracy: 0.8929\n",
      "Epoch 5/100\n",
      "23/23 [==============================] - 371s 16s/step - loss: 2.5745 - accuracy: 0.8080 - val_loss: 2.0909 - val_accuracy: 0.8393\n",
      "Epoch 6/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 3.2661 - accuracy: 0.7812 INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 377s 16s/step - loss: 3.2661 - accuracy: 0.7812 - val_loss: 0.9618 - val_accuracy: 0.9464\n",
      "Epoch 7/100\n",
      "23/23 [==============================] - 370s 16s/step - loss: 2.4926 - accuracy: 0.8259 - val_loss: 1.6419 - val_accuracy: 0.8929\n",
      "Epoch 8/100\n",
      "23/23 [==============================] - 370s 16s/step - loss: 2.2648 - accuracy: 0.8438 - val_loss: 1.0808 - val_accuracy: 0.9464\n",
      "Epoch 9/100\n",
      "23/23 [==============================] - 370s 16s/step - loss: 1.4164 - accuracy: 0.8705 - val_loss: 1.1514 - val_accuracy: 0.9464\n",
      "Epoch 10/100\n",
      "23/23 [==============================] - 368s 16s/step - loss: 1.4719 - accuracy: 0.8750 - val_loss: 1.4548 - val_accuracy: 0.8750\n",
      "Epoch 11/100\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.7260 - accuracy: 0.8661 INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: convnet_pretrained_augmentation.tf\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 373s 16s/step - loss: 1.7260 - accuracy: 0.8661 - val_loss: 0.8118 - val_accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "raw_data, raw_label = get_data(\"train\")\n",
    "\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(raw_data, raw_label, train_size=0.8)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(filepath=\"convnet_pretrained_augmentation.tf\", save_best_only=True, monitor=\"val_loss\"),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5) #Stoppen bij gebrek aan verbetering\n",
    "]\n",
    "\n",
    "history = model.fit(data_train, np.array(labels_train), validation_data=(data_val, np.array(labels_val)), epochs=100, batch_size=10, callbacks=callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T17:26:46.563139100Z",
     "start_time": "2024-02-11T16:18:56.793613400Z"
    }
   },
   "id": "dc79e3fb200f9c74",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "# learning stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0b2833446221a2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[0;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39msubplot(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mhistory\u001B[49m\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mplot(history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x400 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-11T18:32:06.807526900Z",
     "start_time": "2024-02-11T18:32:06.251410400Z"
    }
   },
   "id": "5207bcfa97b2dd98",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c81e81e85e4034a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = keras.models.load_model('convnet_pretrained_augmentation.tf')\n",
    "\n",
    "raw_data, raw_label = get_data(\"test\")\n",
    "\n",
    "# Create predictions.\n",
    "y_pred = model.predict(raw_data)\n",
    "threshold = 0.5\n",
    "y_pred_labels = np.array((y_pred > threshold).astype(int))\n",
    "raw_label = np.array(raw_label)\n",
    "\n",
    "raw_label = np.array(raw_label.argmax(axis=1))\n",
    "y_pred = np.array(y_pred.argmax(axis=1))\n",
    "\n",
    "cm = confusion_matrix(raw_label, y_pred)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-11T18:32:08.556459600Z"
    }
   },
   "id": "d2ce3ee3f9f44aad",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
