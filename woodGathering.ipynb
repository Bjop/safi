{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPool2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from PIL import ImageGrab\n",
    "import os\n",
    "from PIL import Image\n",
    "import pyautogui\n",
    "import time\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.853431Z",
     "start_time": "2024-03-24T18:13:13.850331Z"
    }
   },
   "id": "initial_id",
   "execution_count": 812
  },
  {
   "cell_type": "markdown",
   "source": [
    "# env"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "962ab93539a4978a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "keys = [\"z\", \"s\", \"q\", \"d\", \"e\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.873918Z",
     "start_time": "2024-03-24T18:13:13.871435Z"
    }
   },
   "id": "b330cadde711dee5",
   "execution_count": 813
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class CustomEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        self.total_picked_up = 0\n",
    "        self.action_space = Discrete(5)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1080, 1920, 3), dtype=np.uint8)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation = self.get_screen()\n",
    "        \n",
    "        pyautogui.press(keys[action])\n",
    "  \n",
    "        reward = 0\n",
    "        if self.item_found(observation):\n",
    "            reward = 1\n",
    "            self.total_picked_up += 1\n",
    "            \n",
    "        info = {}\n",
    "        \n",
    "        done = self.total_picked_up >= 5\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_picked_up = 0\n",
    "        return self.total_picked_up\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def get_screen(self):\n",
    "        screen = pyautogui.screenshot()\n",
    "        return np.array(screen)\n",
    "    \n",
    "    def item_found(self, observation):\n",
    "        \n",
    "        if len(observation.shape) == 3:\n",
    "            observation = cv2.cvtColor(observation, cv2.COLOR_BGR2GRAY)\n",
    "        # Read the template\n",
    "        template = cv2.imread(\"resources/template_matching/leafs.png\", 0)\n",
    "\n",
    "        # Perform template matching\n",
    "        if observation.dtype != np.uint8:\n",
    "            observation = observation.astype(np.uint8)\n",
    "        if template.dtype != np.uint8:\n",
    "            template = template.astype(np.uint8)\n",
    "\n",
    "        res = cv2.matchTemplate(observation, template, cv2.TM_CCOEFF_NORMED)\n",
    "        return np.where(res >= .8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.878721Z",
     "start_time": "2024-03-24T18:13:13.873918Z"
    }
   },
   "id": "d6beade834106d04",
   "execution_count": 814
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0b27508b8be172"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Define loss and optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# \n",
    "# # Number of episodes to play\n",
    "# num_episodes = 500\n",
    "# \n",
    "# \n",
    "# # Function to preprocess images\n",
    "# def preprocess_image(image):\n",
    "#     return tf.image.rgb_to_grayscale(image)\n",
    "# \n",
    "# \n",
    "# for episode in range(num_episodes):\n",
    "#     initial_state = env.reset()\n",
    "# \n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = env.action_space.sample()\n",
    "#         next_state, reward, done, info = env.step(action)\n",
    "# \n",
    "#         # next_state = preprocess_image(next_state)\n",
    "# \n",
    "#         # training here\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.882487Z",
     "start_time": "2024-03-24T18:13:13.880727Z"
    }
   },
   "id": "ddacd382ee6712b9",
   "execution_count": 815
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # Capture screen\n",
    "#     screen = pyautogui.screenshot()\n",
    "#     # Convert the image into numpy array representation\n",
    "#     frame = np.array(screen)\n",
    "#     # Convert the BGR image into RGB image\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     # Resize the capturing screen to 1080p\n",
    "#     frame = cv2.resize(frame, (1920, 1080))\n",
    "#     # Display screen in 1080p\n",
    "#     cv2.imshow('Screen Capture in 1080p', frame)\n",
    "# \n",
    "#     # Wait for the user to press the ESC key (ASCII 27) to quit capturing the screen\n",
    "#     if cv2.waitKey(1) == 27:\n",
    "#         break\n",
    "# \n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.885245Z",
     "start_time": "2024-03-24T18:13:13.883491Z"
    }
   },
   "id": "3fccfe8ced41e9ef",
   "execution_count": 816
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1334442006ded5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Define the Convolutional Neural Network\n",
    "# inputs = layers.Input(shape=states)\n",
    "# x = layers.Conv2D(32, (8, 8), strides=4, activation='relu')(inputs)\n",
    "# x = layers.Conv2D(64, (4, 4), strides=2, activation='relu')(x)\n",
    "# x = layers.Conv2D(64, (3, 3), strides=1, activation='relu')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(512, activation='relu')(x)\n",
    "# outputs = layers.Dense(env.action_space.n, activation='linear')(x)\n",
    "# \n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.887880Z",
     "start_time": "2024-03-24T18:13:13.885245Z"
    }
   },
   "id": "fd7ed4711d3123c0",
   "execution_count": 817
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model.build(states)\n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.890182Z",
     "start_time": "2024-03-24T18:13:13.887880Z"
    }
   },
   "id": "4eb19cb016680727",
   "execution_count": 818
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create an ampty .keras file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "432ce2f6931428a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DQNN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa67873bc969ce2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q-learning NN class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b03ba7e152d0aee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self, input_shape, out_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv2D(32, (8, 8), strides=4, activation='relu')(inputs)\n",
    "        x = layers.Conv2D(64, (4, 4), strides=2, activation='relu')(x)\n",
    "        x = layers.Conv2D(64, (3, 3), strides=1, activation='relu')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        output = layers.Dense(units=out_actions, activation='linear')(x)\n",
    "        self.model = keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        self.model.build(self.model)\n",
    "        self.model.summary()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        return self.model(input_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.893833Z",
     "start_time": "2024-03-24T18:13:13.890182Z"
    }
   },
   "id": "113c0b50d61cc899",
   "execution_count": 819
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define memory for Experience Replay\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.896626Z",
     "start_time": "2024-03-24T18:13:13.893833Z"
    }
   },
   "id": "a46b4fd0260388b2",
   "execution_count": 820
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FrozenLakeDQL():\n",
    "    # Hyperparameters (adjustable)\n",
    "    learning_rate_a = 0.001         # learning rate (alpha)\n",
    "    discount_factor_g = 0.9         # discount rate (gamma)\n",
    "    network_sync_rate = 10          # number of steps the agent takes before syncing the policy and target network\n",
    "    replay_memory_size = 1000       # size of replay memory\n",
    "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
    "\n",
    "    # Neural Network\n",
    "    loss_fn = keras.losses.MeanSquaredError()         # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "    optimizer = None                # NN Optimizer. Initialize later.\n",
    "\n",
    "    ACTIONS = ['z', 'q', 's', 'd', 'e']     # for printing 0,1,2,3,4 => Z(forwards), Q(left), S(backwards),D(right), E(action)\n",
    "\n",
    "    # Train the FrozeLake environment\n",
    "    def train(self, episodes):\n",
    "        # Create FrozenLake instance\n",
    "        env = CustomEnv()\n",
    "        num_states = env.observation_space\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        epsilon = 1 # 1 = 100% random actions\n",
    "        memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "        # Create policy and target network.\n",
    "        policy_dqn = DQN(input_shape=(1920, 1080, 3), out_actions=num_actions)\n",
    "        target_dqn = DQN(input_shape=(1920, 1080, 3), out_actions=num_actions)\n",
    "\n",
    "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
    "        target_dqn.set_weights(policy_dqn.get_weights())\n",
    "\n",
    "        print('Policy (random, before training):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_a)\n",
    "\n",
    "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
    "        rewards_per_episode = np.zeros(episodes)\n",
    "\n",
    "        # List to keep track of epsilon decay\n",
    "        epsilon_history = []\n",
    "\n",
    "        # Track number of steps taken. Used for syncing policy => target network.\n",
    "        step_count=0\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
    "                else:\n",
    "                    # select best action\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state))\n",
    "\n",
    "                # Execute action\n",
    "                new_state,reward,terminated,truncated = env.step(action)\n",
    "\n",
    "                # Save experience into memory\n",
    "                memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "                # Increment step counter\n",
    "                step_count+=1\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            if reward == 1:\n",
    "                rewards_per_episode[i] = 1\n",
    "\n",
    "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
    "            if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
    "                mini_batch = memory.sample(self.mini_batch_size)\n",
    "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.set_weights(policy_dqn.get_weights)\n",
    "                    step_count=0\n",
    "\n",
    "        # Close environment\n",
    "        env.close()\n",
    "\n",
    "        # Save policy\n",
    "        policy_dqn.save_weights(\"woodGathering_dql_cnn.keras\")\n",
    "\n",
    "        # Create new graph\n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        sum_rewards = np.zeros(episodes)\n",
    "        for x in range(episodes):\n",
    "            sum_rewards[x] = np.sum(rewards_per_episode[max(0, x-100):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        plt.plot(sum_rewards)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        # Save plots\n",
    "        plt.savefig('woodGathering_dql_cnn.png')\n",
    "\n",
    "    # Optimize policy network\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "\n",
    "        for state, action, new_state, reward, terminated in mini_batch:\n",
    "\n",
    "            if terminated:\n",
    "                # Agent either reached goal (reward=1) or fell into hole (reward=0)\n",
    "                # When in a terminated state, target q value should be set to the reward.\n",
    "                target = np.array([reward])\n",
    "            else:\n",
    "                # Calculate target q value\n",
    "                target = np.array(\n",
    "                    reward + self.discount_factor_g * tf.reduce_max(target_dqn(self.state_to_dqn_input(new_state)))\n",
    "                )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state))\n",
    "\n",
    "            # Adjust the specific action to the target that was just calculated. \n",
    "            # Target_q[batch][action], hardcode batch to 0 because there is only 1 batch.\n",
    "            target_q_var = tf.Variable(target_q.numpy())\n",
    "            target_q_var[0, action].assign(target)\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(tf.stack(current_q_list), tf.stack(target_q_list))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def state_to_dqn_input(self, state):\n",
    "        image = tf.zeros((1920, 1080, 3), dtype=tf.uint8)\n",
    "        image = np.reshape(image, (1920, 1080, 3))\n",
    "        \n",
    "        return np.array([image])\n",
    "\n",
    "    # Run the FrozeLake environment with the learned policy\n",
    "    def test(self, episodes):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human')\n",
    "        num_states = env.observation_space.n\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(input_shape=3, out_actions=num_actions)\n",
    "        policy_dqn.load_weights(\"woodGathering_dql_cnn.keras\")\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        print('Policy (trained):')\n",
    "        self.print_dqn(policy_dqn)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions\n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):\n",
    "                # Select best action\n",
    "                action = tf.argmax(policy_dqn(self.state_to_dqn_input(state)), axis=-1).numpy()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # Print DQN: state, best action, q values\n",
    "    def print_dqn(self, dqn):\n",
    "        # Loop each state and print policy to console\n",
    "        for s in range(16):\n",
    "            #  Format q values for printing\n",
    "            q_values = ''\n",
    "            for q in dqn(self.state_to_dqn_input(s))[0]:\n",
    "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
    "            q_values=q_values.rstrip()              # Remove space at the end\n",
    "\n",
    "            # Map the best action to L D R U\n",
    "            print(f'x {np.argmax(dqn(self.state_to_dqn_input(s)))}')\n",
    "            best_action = self.ACTIONS[np.argmax(dqn(self.state_to_dqn_input(s)))]\n",
    "\n",
    "            # Print policy in the format of: state, action, q values\n",
    "            # The printed layout matches the FrozenLake map.\n",
    "            print(f'{s:02},{best_action},[{q_values}]', end=' ')\n",
    "            if (s+1)%4==0:\n",
    "                print() # Print a newline every 4 states"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:13.906698Z",
     "start_time": "2024-03-24T18:13:13.896626Z"
    }
   },
   "id": "dcf7ed64dbadb2a2",
   "execution_count": 821
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4cfbafc48fce87fc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"functional_447\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_447\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_234 (\u001B[38;5;33mInputLayer\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1920\u001B[0m, \u001B[38;5;34m1080\u001B[0m, \u001B[38;5;34m3\u001B[0m)  │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_719 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m479\u001B[0m, \u001B[38;5;34m269\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │         \u001B[38;5;34m6,176\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_720 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m238\u001B[0m, \u001B[38;5;34m133\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m32,832\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_721 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m236\u001B[0m, \u001B[38;5;34m131\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m36,928\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_135 (\u001B[38;5;33mFlatten\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1978624\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_256 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m)              │     \u001B[38;5;34m9,893,125\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_234 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1080</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_719 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">479</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">269</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_720 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">238</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">133</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_721 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">236</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">131</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1978624</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_256 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,893,125</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m9,969,061\u001B[0m (38.03 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,969,061</span> (38.03 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m9,969,061\u001B[0m (38.03 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,969,061</span> (38.03 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"functional_449\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_449\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_235 (\u001B[38;5;33mInputLayer\u001B[0m)    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1920\u001B[0m, \u001B[38;5;34m1080\u001B[0m, \u001B[38;5;34m3\u001B[0m)  │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_722 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m479\u001B[0m, \u001B[38;5;34m269\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │         \u001B[38;5;34m6,176\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_723 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m238\u001B[0m, \u001B[38;5;34m133\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m32,832\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_724 (\u001B[38;5;33mConv2D\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m236\u001B[0m, \u001B[38;5;34m131\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m36,928\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_136 (\u001B[38;5;33mFlatten\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1978624\u001B[0m)        │             \u001B[38;5;34m0\u001B[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_257 (\u001B[38;5;33mDense\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m5\u001B[0m)              │     \u001B[38;5;34m9,893,125\u001B[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_235 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1080</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_722 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">479</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">269</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_723 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">238</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">133</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ conv2d_724 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">236</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">131</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1978624</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_257 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,893,125</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m9,969,061\u001B[0m (38.03 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,969,061</span> (38.03 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m9,969,061\u001B[0m (38.03 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,969,061</span> (38.03 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy (random, before training):\n",
      "x 0\n",
      "00,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "01,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "02,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "03,z,[+0.00 +0.00 +0.00 +0.00 +0.00] \n",
      "x 0\n",
      "04,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "05,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "06,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "07,z,[+0.00 +0.00 +0.00 +0.00 +0.00] \n",
      "x 0\n",
      "08,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "09,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "10,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "11,z,[+0.00 +0.00 +0.00 +0.00 +0.00] \n",
      "x 0\n",
      "12,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "13,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "14,z,[+0.00 +0.00 +0.00 +0.00 +0.00] x 0\n",
      "15,z,[+0.00 +0.00 +0.00 +0.00 +0.00] \n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,1,5] vs. [0] [Op:Sub] name: ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[822], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m frozen_lake \u001B[38;5;241m=\u001B[39m FrozenLakeDQL()\n\u001B[0;32m      4\u001B[0m is_slippery \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mfrozen_lake\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m frozen_lake\u001B[38;5;241m.\u001B[39mtest(\u001B[38;5;241m10\u001B[39m)\n",
      "Cell \u001B[1;32mIn[821], line 82\u001B[0m, in \u001B[0;36mFrozenLakeDQL.train\u001B[1;34m(self, episodes)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(memory)\u001B[38;5;241m>\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmini_batch_size \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39msum(rewards_per_episode)\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     81\u001B[0m     mini_batch \u001B[38;5;241m=\u001B[39m memory\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmini_batch_size)\n\u001B[1;32m---> 82\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmini_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy_dqn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_dqn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;66;03m# Decay epsilon\u001B[39;00m\n\u001B[0;32m     85\u001B[0m     epsilon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(epsilon \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39mepisodes, \u001B[38;5;241m0\u001B[39m)\n",
      "Cell \u001B[1;32mIn[821], line 147\u001B[0m, in \u001B[0;36mFrozenLakeDQL.optimize\u001B[1;34m(self, mini_batch, policy_dqn, target_dqn)\u001B[0m\n\u001B[0;32m    144\u001B[0m     target_q_var[\u001B[38;5;241m0\u001B[39m, action]\u001B[38;5;241m.\u001B[39massign(target)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;66;03m# Compute loss for the whole minibatch\u001B[39;00m\n\u001B[1;32m--> 147\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_q_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_q_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;66;03m# Optimize the model\u001B[39;00m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\loss.py:43\u001B[0m, in \u001B[0;36mLoss.__call__\u001B[1;34m(self, y_true, y_pred, sample_weight)\u001B[0m\n\u001B[0;32m     36\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype), y_pred\n\u001B[0;32m     38\u001B[0m )\n\u001B[0;32m     39\u001B[0m y_true \u001B[38;5;241m=\u001B[39m tree\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x: ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype), y_true\n\u001B[0;32m     41\u001B[0m )\n\u001B[1;32m---> 43\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     44\u001B[0m out_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(losses, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_keras_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m out_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\losses.py:22\u001B[0m, in \u001B[0;36mLossFunctionWrapper.call\u001B[1;34m(self, y_true, y_pred)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall\u001B[39m(\u001B[38;5;28mself\u001B[39m, y_true, y_pred):\n\u001B[0;32m     21\u001B[0m     y_true, y_pred \u001B[38;5;241m=\u001B[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1154\u001B[0m, in \u001B[0;36mmean_squared_error\u001B[1;34m(y_true, y_pred)\u001B[0m\n\u001B[0;32m   1152\u001B[0m y_true \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_to_tensor(y_true, dtype\u001B[38;5;241m=\u001B[39my_pred\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m   1153\u001B[0m y_true, y_pred \u001B[38;5;241m=\u001B[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001B[1;32m-> 1154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ops\u001B[38;5;241m.\u001B[39mmean(ops\u001B[38;5;241m.\u001B[39msquare(\u001B[43my_true\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m), axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5983\u001B[0m, in \u001B[0;36mraise_from_not_ok_status\u001B[1;34m(e, name)\u001B[0m\n\u001B[0;32m   5981\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_from_not_ok_status\u001B[39m(e, name) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NoReturn:\n\u001B[0;32m   5982\u001B[0m   e\u001B[38;5;241m.\u001B[39mmessage \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m name: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m-> 5983\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: {{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [32,1,5] vs. [0] [Op:Sub] name: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    frozen_lake = FrozenLakeDQL()\n",
    "    is_slippery = False\n",
    "    frozen_lake.train(1000)\n",
    "    frozen_lake.test(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T18:13:25.269337Z",
     "start_time": "2024-03-24T18:13:13.908139Z"
    }
   },
   "id": "e6a51d1224e74671",
   "execution_count": 822
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
